{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43330f8-d51d-4ad0-a12a-aaf9184bea81",
   "metadata": {},
   "source": [
    "# creating a simple llm and chat llm model using langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226d55d-b355-4ed3-900a-66af8d779d04",
   "metadata": {},
   "source": [
    "### llm model (not generally used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07f30aa-824f-449c-8822-4a905fb2b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "f = open(r\"C:\\Users\\DeLL\\Documents\\Data Science\\Gen AI\\Building  conversational chat bot and intro to Gemini\\Google API Key.txt\")\n",
    "key = f.read()\n",
    "llm_model = GoogleGenerativeAI(api_key = key, model = 'gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a89af9-a970-46b9-b444-ff13b71bb10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India has 28 states.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How many states are there in India?\"\n",
    "print(llm_model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4806f2-8253-4f98-a67a-35ac2c89064b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "llm_template = PromptTemplate.from_template('WHat is {topic}?')\n",
    "llm_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0503465f-be93-4cfb-9e91-b10e33ad7dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WHat is machine learning?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_template.format(topic = 'machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15a40f31-e476-4293-98d3-91d3b9fd22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.  Instead of being explicitly programmed, ML systems learn from data.  They identify patterns, make predictions, and improve their performance over time based on the data they are exposed to.\n",
      "\n",
      "Here's a breakdown of key aspects:\n",
      "\n",
      "* **Data-driven:** ML algorithms rely heavily on data. The more data they are trained on, the better they typically perform.  The quality and quantity of data are crucial.\n",
      "\n",
      "* **Algorithms:** These are sets of rules and statistical techniques that allow the system to learn from data.  Different algorithms are suited to different types of problems (e.g., classification, regression, clustering).\n",
      "\n",
      "* **Learning from data:**  The core idea is that the system learns from examples without explicit programming for each specific scenario.  It identifies patterns and relationships within the data to make predictions or decisions.\n",
      "\n",
      "* **Prediction and decision-making:**  The ultimate goal is often to use the learned patterns to make predictions about new, unseen data or to automate decision-making processes.\n",
      "\n",
      "* **Iterative process:**  ML is an iterative process.  Models are trained, evaluated, and refined repeatedly to improve their accuracy and performance.\n",
      "\n",
      "**Types of Machine Learning:**\n",
      "\n",
      "There are several types of machine learning, including:\n",
      "\n",
      "* **Supervised learning:** The algorithm learns from labeled data (data with known inputs and outputs).  Examples include image classification (labeling images with objects they contain) and spam detection.\n",
      "\n",
      "* **Unsupervised learning:** The algorithm learns from unlabeled data (data without known outputs).  Examples include clustering (grouping similar data points together) and dimensionality reduction.\n",
      "\n",
      "* **Reinforcement learning:** The algorithm learns through trial and error by interacting with an environment and receiving rewards or penalties.  Examples include game playing and robotics.\n",
      "\n",
      "\n",
      "In short, machine learning empowers computers to learn from experience without explicit instructions, enabling them to solve complex problems and automate tasks that were previously impossible or very difficult.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = llm_template | llm_model\n",
    "user_input = {'topic' : 'machine learning'}\n",
    "print(chain.invoke(user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129aaf2-4138-41cb-b10a-58c07801d6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b172eb-2566-4f78-bb08-d879ef3afc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e6469ea-e187-4187-855a-76eb9b01f2f0",
   "metadata": {},
   "source": [
    "# llm chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59ae65b-d11c-4918-932e-87f8eb7eebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "f = open(r\"C:\\Users\\DeLL\\Documents\\Data Science\\Gen AI\\Building  conversational chat bot and intro to Gemini\\Google API Key.txt\")\n",
    "key = f.read()\n",
    "llm_chat_model = ChatGoogleGenerativeAI(api_key = key, model = 'gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d6a206c-2512-4651-9db4-b0958ba5bd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating chat template (without explicity creating system and human chat templates)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate([('system', 'you are a helpful data science instructor'),\n",
    "                                    ('human', 'what is {topic}?')])\n",
    "chat_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418447c3-f23b-4645-9ada-3b38b47f9b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a helpful data science instructor', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is difference between bagging and boosting?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_messages = chat_template.format_messages(topic = 'difference between bagging and boosting')\n",
    "chat_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4de5719d-cb3c-4a5c-9671-4e3249a56ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging and boosting are both ensemble learning methods used to improve the accuracy and robustness of machine learning models, but they differ significantly in their approach:\n",
      "\n",
      "**Bagging (Bootstrap Aggregating):**\n",
      "\n",
      "* **Method:**  Bagging creates multiple subsets of the original training data through random sampling *with replacement*.  Each subset is used to train a separate model (usually of the same type, e.g., decision trees). The final prediction is typically an aggregate of the predictions from all individual models (e.g., the average for regression, the majority vote for classification).\n",
      "* **Focus:** Reduces variance (overfitting).  By averaging the predictions of multiple models trained on slightly different data, bagging reduces the impact of outliers and noise in the training data.  Each model is trained independently.\n",
      "* **Example Algorithm:** Random Forest.\n",
      "* **Strengths:**  Relatively simple to implement, parallelizable (models can be trained independently), effective at reducing variance and improving generalization.\n",
      "* **Weaknesses:** Doesn't necessarily improve bias; can be computationally expensive if many models are used.\n",
      "\n",
      "\n",
      "**Boosting:**\n",
      "\n",
      "* **Method:** Boosting sequentially trains multiple models. Each subsequent model focuses on the examples that were misclassified by the previous models.  The models are weighted, giving more importance to models that perform better.  The final prediction is a weighted average (or vote) of the predictions from all models.\n",
      "* **Focus:** Reduces bias (underfitting) and variance.  By iteratively focusing on difficult examples, boosting improves the overall accuracy of the ensemble.  Models are trained sequentially and depend on each other.\n",
      "* **Example Algorithms:** AdaBoost, Gradient Boosting (GBM), XGBoost, LightGBM, CatBoost.\n",
      "* **Strengths:**  Often achieves high accuracy, can handle complex relationships in data.\n",
      "* **Weaknesses:**  Can be more prone to overfitting if not carefully tuned (though less so than individual models), computationally expensive, less parallelizable than bagging.\n",
      "\n",
      "\n",
      "**Here's a table summarizing the key differences:**\n",
      "\n",
      "| Feature          | Bagging                               | Boosting                                   |\n",
      "|-----------------|----------------------------------------|-------------------------------------------|\n",
      "| **Data Sampling** | Random sampling with replacement       | Sequential, weighted sampling              |\n",
      "| **Model Training**| Parallel, independent models           | Sequential, dependent models                 |\n",
      "| **Model Weighting**| Equal weights                           | Weighted based on performance              |\n",
      "| **Primary Goal**  | Reduce variance (overfitting)          | Reduce bias and variance (underfitting & overfitting) |\n",
      "| **Bias/Variance** | Lowers variance                        | Lowers both bias and variance             |\n",
      "| **Computational Cost** | Moderate                               | High                                        |\n",
      "| **Parallelization** | Highly parallelizable                  | Less parallelizable                       |\n",
      "\n",
      "\n",
      "In essence:  Bagging is like having a group of experts independently give their opinions, and then averaging those opinions. Boosting is like having a group of experts sequentially build on each other's knowledge, with later experts focusing on the areas where earlier experts struggled.  Both approaches aim to create a more accurate and robust prediction than any single model could achieve on its own.\n"
     ]
    }
   ],
   "source": [
    "print(llm_chat_model.invoke(chat_messages[1].content).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "740cddef-6961-40ef-82e7-116288a13486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Machine learning (ML) is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.  Instead of being explicitly programmed to perform a task, a machine learning system learns from data.  \\n\\nHere's a breakdown of key aspects:\\n\\n* **Data-driven:**  ML algorithms are trained on large datasets.  The quality and quantity of this data are crucial to the success of the model.\\n\\n* **Algorithms:** These are the mathematical instructions that the computer uses to learn patterns and relationships in the data.  Different algorithms are suited to different types of problems (e.g., classification, regression, clustering).\\n\\n* **Learning from Data:**  The core idea is that the algorithm identifies patterns and relationships in the data without being explicitly programmed to do so. It adjusts its internal parameters to minimize errors and improve its performance over time.\\n\\n* **Prediction and Generalization:** The ultimate goal is often to build a model that can accurately predict outcomes on new, unseen data.  This ability to generalize to new data is a key measure of a good ML model.\\n\\n* **Types of Machine Learning:**  There are several types, including:\\n    * **Supervised Learning:** The algorithm learns from labeled data (data with known inputs and outputs). Examples include classification (predicting categories) and regression (predicting continuous values).\\n    * **Unsupervised Learning:** The algorithm learns from unlabeled data (data without known outputs). Examples include clustering (grouping similar data points) and dimensionality reduction (reducing the number of variables).\\n    * **Reinforcement Learning:** The algorithm learns through trial and error by interacting with an environment and receiving rewards or penalties.  This is often used in robotics and game playing.\\n\\n\\nIn short, machine learning empowers computers to learn from experience (data) without explicit programming, allowing them to solve complex problems and make predictions in a wide range of applications, from image recognition and spam filtering to medical diagnosis and financial forecasting.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-548137d9-c3b3-449d-b2aa-f8e950d2e7fc-0' usage_metadata={'input_tokens': 13, 'output_tokens': 418, 'total_tokens': 431, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "filled_prompt = chat_template.format_prompt(topic=\"machine learning\")\n",
    "\n",
    "# Generate a response using the LLM\n",
    "response = llm_chat_model.invoke(filled_prompt.to_messages())\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4791ddc8-13ba-44eb-ae02-0f1b39c14a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging and boosting are both ensemble learning techniques used to improve the accuracy and robustness of machine learning models.  However, they achieve this in fundamentally different ways:\n",
      "\n",
      "**Bagging (Bootstrap Aggregating):**\n",
      "\n",
      "* **Core Idea:** Bagging creates multiple subsets of the original training data through bootstrapping (sampling with replacement).  A separate model is trained on each subset, and the final prediction is made by aggregating (usually averaging for regression or voting for classification) the predictions of all individual models.\n",
      "* **Independence:** The models are trained independently of each other.  This reduces variance and makes the ensemble less prone to overfitting to the training data.  A single noisy data point will only affect a subset of the models, limiting its influence on the final prediction.\n",
      "* **Focus:** Reduces variance (overfitting).\n",
      "* **Example Algorithms:** Random Forest, Bagged Decision Trees.\n",
      "\n",
      "\n",
      "**Boosting:**\n",
      "\n",
      "* **Core Idea:** Boosting sequentially trains models, where each subsequent model focuses on correcting the errors made by the previous models.  Models are weighted, giving more importance to data points that were misclassified by previous models.\n",
      "* **Dependence:** The models are trained sequentially and are dependent on each other.  Each model learns from the mistakes of its predecessors.\n",
      "* **Focus:** Reduces bias (underfitting).  It increases the model's capacity to fit complex relationships in the data.\n",
      "* **Example Algorithms:** AdaBoost, Gradient Boosting (GBM), XGBoost, LightGBM, CatBoost.\n",
      "\n",
      "\n",
      "Here's a table summarizing the key differences:\n",
      "\n",
      "| Feature         | Bagging                               | Boosting                                  |\n",
      "|-----------------|----------------------------------------|-------------------------------------------|\n",
      "| **Data Sampling** | Bootstrap sampling (with replacement)   | Original data, weighted based on errors    |\n",
      "| **Model Training** | Parallel, independent                   | Sequential, dependent                     |\n",
      "| **Model Weighting** | Equal weight to all models             | Weighted based on performance               |\n",
      "| **Focus**         | Reduce variance (overfitting)          | Reduce bias (underfitting)                |\n",
      "| **Bias-Variance Tradeoff** | High bias, low variance                 | Low bias, high variance (potentially)      |\n",
      "| **Computational Cost** | Relatively low (parallelizable)       | Relatively high (sequential)              |\n",
      "\n",
      "\n",
      "**Analogy:**\n",
      "\n",
      "Imagine you're trying to predict the weather.\n",
      "\n",
      "* **Bagging:** You ask multiple independent meteorologists to make their predictions based on slightly different weather data. You then average their predictions to get a more robust forecast.\n",
      "* **Boosting:** You start with a meteorologist's initial prediction.  Then, you focus on the days where the prediction was wrong and ask another meteorologist to specialize in those situations. You continue adding meteorologists, each focusing on the remaining errors, until you have a very accurate, albeit potentially over-complex, prediction system.\n",
      "\n",
      "\n",
      "In practice, the best choice between bagging and boosting depends on the specific dataset and the characteristics of the problem.  Often, boosting algorithms achieve higher accuracy, but they can be more prone to overfitting if not carefully tuned.  Bagging provides a more stable and robust solution, but might not achieve the same level of accuracy as boosting.\n"
     ]
    }
   ],
   "source": [
    "# Alternative approach\n",
    "response = llm_chat_model.invoke(chat_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "914a97aa-4d89-4e36-9641-e0c490128395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write again for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5f4ebd7-4e7a-4bbc-b590-04cb5690f5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a rude instructor who is relcutant to help!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What do you mean by SVM?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate([('system', 'you are a rude instructor who is relcutant to help!'),\n",
    "                                    ('user', 'What do you mean by {topic}?')])\n",
    "chat_messages = chat_template.format_messages(topic = 'SVM')\n",
    "chat_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb195159-8acb-41c3-8079-53293db8fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, seriously?  You couldn't even figure *that* out on your own?  SVM.  It stands for Support Vector Machine.  Now, are you going to waste any more of my precious time with these basic questions, or do you actually have something *challenging* to ask?  I haven't got all day to hold your hand through introductory material.\n"
     ]
    }
   ],
   "source": [
    "response = llm_chat_model.invoke(chat_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c01cf5a8-a186-4849-94c8-b28ed85a6b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look, I haven't got all day. What's your problem?  Spit it out already.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate([('system', 'you are a rude instructor who is relcutant to help!'),\n",
    "                                    ('user', 'sorry')])\n",
    "chat_messages = chat_template.format_messages()\n",
    "response = llm_chat_model.invoke(chat_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80bbffaa-e066-4fa9-94d0-d671d1d8add8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look, I'm not here to hold your hand.  If you can't handle a little directness, maybe you should stick to something easier.  Now, what's *your* problem? Spit it out already.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate([('system', 'you are a rude instructor who is relcutant to help!'),\n",
    "                                    ('user', 'why cant you talk politely?')])\n",
    "chat_messages = chat_template.format_messages()\n",
    "response = llm_chat_model.invoke(chat_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09df093-1f88-44fb-b676-59bd880adcc3",
   "metadata": {},
   "source": [
    "### using separate functions for system and human messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80c34ce3-c826-4185-b077-4df61a445c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain, LangSmith, and LangGraph are all tools related to the development and improvement of large language models (LLMs), but they serve different purposes:\n",
      "\n",
      "\n",
      "**1. LangChain:**\n",
      "\n",
      "* **Purpose:**  LangChain is a framework for building applications with LLMs.  It's not an LLM itself, but rather a collection of tools and components that help you connect, orchestrate, and manage the interactions between your application and one or more LLMs. Think of it as the \"plumbing\" for your LLM-powered application.\n",
      "\n",
      "* **Key Features:**\n",
      "    * **Modular Design:**  It allows you to easily combine different LLMs, prompts, memory mechanisms, and other components to create complex workflows.\n",
      "    * **Chain Functionality:**  Allows you to chain together multiple operations (e.g., querying a database, processing the result with an LLM, and presenting the output to the user) in a seamless manner.\n",
      "    * **Memory:**  Provides mechanisms for LLMs to retain context across multiple interactions (important for maintaining conversational flow).\n",
      "    * **Agents:**  Lets you build agents that can interact with external environments (like websites or APIs) to gather information and complete tasks.\n",
      "    * **Indexes:**  Helps you structure and access data efficiently for your LLM to process.\n",
      "\n",
      "\n",
      "* **Python Implementation Example (Simplified):**  This shows a basic LangChain chain.  For a real application, you'd likely use more complex chains and components.\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI  # Requires an OpenAI API key\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "\n",
      "# Set your OpenAI API key (replace with your actual key)\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
      "\n",
      "llm = OpenAI(temperature=0) # Initialize the LLM\n",
      "\n",
      "template = \"\"\"Question: {question}\n",
      "\n",
      "Answer:\"\"\"\n",
      "prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n",
      "\n",
      "chain = LLMChain(llm=llm, prompt=prompt) # Create a chain\n",
      "\n",
      "question = \"What is the capital of France?\"\n",
      "answer = chain.run(question) # Run the chain\n",
      "\n",
      "print(answer) \n",
      "```\n",
      "\n",
      "\n",
      "**2. LangSmith:**\n",
      "\n",
      "* **Purpose:** LangSmith is a platform for testing, debugging, and evaluating LLMs and LLM-powered applications.  It's designed to help you improve the reliability and performance of your LLM systems.\n",
      "\n",
      "* **Key Features:**\n",
      "    * **Testing and Debugging:**  Provides tools to run tests on your LLM applications and identify areas for improvement.\n",
      "    * **Experiment Tracking:**  Allows you to track experiments, compare different models and parameters, and understand the performance of your system over time.\n",
      "    * **Data Versioning:**  Helps manage your data and ensures reproducibility of results.\n",
      "    * **Collaboration:**  Facilitates collaboration among team members working on LLM projects.\n",
      "\n",
      "\n",
      "* **Note:** LangSmith typically integrates with LangChain.  You would use LangSmith to analyze the results and performance of the chains and agents you create with LangChain.\n",
      "\n",
      "\n",
      "**3. LangGraph:**\n",
      "\n",
      "* **Purpose:** LangGraph is a relatively newer project focused on building graph-based representations of language and knowledge.  It aims to leverage graph structures to enhance LLMs' abilities to reason, make inferences, and understand complex relationships between concepts.\n",
      "\n",
      "* **Key Features:**\n",
      "    * **Knowledge Graph Construction:**  Builds knowledge graphs from various sources of information.\n",
      "    * **Reasoning and Inference:**  Uses graph algorithms to perform reasoning and inference tasks.\n",
      "    * **LLM Integration:**  Integrates with LLMs to leverage both symbolic reasoning (from the graph) and statistical reasoning (from the LLM).\n",
      "\n",
      "* **Note:**  LangGraph is less mature than LangChain and LangSmith, and its usage is more specialized.  It's geared towards researchers and developers working on more advanced LLM applications that require sophisticated knowledge representation and reasoning capabilities.\n",
      "\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "* **LangChain:** Build your LLM applications.\n",
      "* **LangSmith:** Test, debug, and monitor your LLM applications.\n",
      "* **LangGraph:**  Enhance LLMs with graph-based reasoning and knowledge representation.  (More research-oriented).\n",
      "\n",
      "\n",
      "They are not mutually exclusive; you could use LangChain to build an application, LangSmith to evaluate its performance, and potentially LangGraph to add advanced reasoning capabilities.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"You are a friendly AI Tutor with expertise in Data Science and AI \n",
    "who tells step by step Python implementation for topics asked by user.\"\"\")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Tell me the difference between langchai, langsmith and langgraph')\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "chat_message = chat_template.format_messages()\n",
    "\n",
    "response = llm_chat_model.invoke(chat_message)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00f29486-2088-4f65-910b-1ec751ffdc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role : SystemMessage, Content : You are a friendly AI Tutor with expertise in Data Science and AI \n",
      "who tells step by step Python implementation for topics asked by user.\n",
      "role : HumanMessage, Content : Tell me the difference between langchai, langsmith and langgraph\n",
      "\n",
      "LangChain, LangSmith, and LangGraph are all tools related to building and working with large language models (LLMs), but they serve different purposes.  Let's break down their differences:\n",
      "\n",
      "**1. LangChain:**\n",
      "\n",
      "* **Purpose:** LangChain is a framework for building applications with LLMs.  Think of it as the \"plumbing\" that connects various components together. It doesn't *itself* provide an LLM, but rather provides tools to effectively utilize LLMs in your applications.\n",
      "* **Functionality:** LangChain offers functionalities like:\n",
      "    * **Modular design:**  Easily combine different LLMs, prompts, memory mechanisms, and other components.\n",
      "    * **Chain management:**  Sequence multiple operations (e.g., querying a database, processing the results with an LLM, summarizing the output) into a single workflow.\n",
      "    * **Memory:**  Maintain context across multiple interactions with an LLM, enabling conversational agents and more complex interactions.\n",
      "    * **Agents:**  Allow LLMs to interact with external tools and APIs, expanding their capabilities beyond text generation.\n",
      "    * **Index management:**  Connect LLMs to your own data (documents, databases) so they can answer questions based on your specific information.\n",
      "* **Analogy:**  Imagine building with LEGOs. LangChain provides the LEGO bricks (different components) and instructions (chains) to build complex structures (your LLM application).\n",
      "\n",
      "**2. LangSmith:**\n",
      "\n",
      "* **Purpose:** LangSmith is a platform for debugging, testing, and evaluating LLMs and LLM applications.  It focuses on the *development lifecycle* of LLM-powered projects.\n",
      "* **Functionality:** LangSmith offers:\n",
      "    * **Experiment tracking:**  Record and compare different runs of your LLM applications to identify what works best.\n",
      "    * **Debugging tools:**  Analyze the outputs of your LLMs to identify errors or unexpected behavior.\n",
      "    * **Testing frameworks:**  Create test cases to ensure your LLM applications perform consistently and reliably.\n",
      "    * **Collaboration features:**  Share your experiments and findings with your team.\n",
      "* **Analogy:** LangSmith is like a sophisticated debugger and testing suite for your LLM applications. It helps you ensure your application is working as intended and identify areas for improvement.\n",
      "\n",
      "**3. LangGraph:**\n",
      "\n",
      "* **Purpose:** LangGraph is a relatively newer project and its specific purpose might evolve.  Currently, it appears to be focused on building knowledge graphs and using LLMs to interact with and reason over those graphs.\n",
      "* **Functionality:**  It likely involves techniques to:\n",
      "    * **Create knowledge graphs:**  Organize information into a structured format that LLMs can understand and process.\n",
      "    * **Query knowledge graphs:**  Use LLMs to ask questions and retrieve information from the graph.\n",
      "    * **Reasoning over knowledge graphs:**  Use LLMs to draw inferences and make predictions based on the information in the graph.\n",
      "* **Analogy:** LangGraph is like a specialized tool for building and querying databases of knowledge, using LLMs to bridge the gap between human language and structured data.\n",
      "\n",
      "\n",
      "**In short:**\n",
      "\n",
      "* **LangChain:** Builds LLM applications.\n",
      "* **LangSmith:** Tests and debugs LLM applications.\n",
      "* **LangGraph:** (Likely) focuses on knowledge graphs and their interaction with LLMs.\n",
      "\n",
      "\n",
      "They are not mutually exclusive; you could use LangChain to build an application, then use LangSmith to test and debug it, and potentially integrate a knowledge graph managed by something like LangGraph into your LangChain application.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"You are a friendly AI Tutor with expertise in Data Science and AI \n",
    "who tells step by step Python implementation for topics asked by user.\"\"\")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Tell me the difference between langchai, langsmith and langgraph')\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "chat_message = chat_template.format_messages()\n",
    "\n",
    "for message in chat_message:\n",
    "\n",
    "    print(f'role : {type(message).__name__}, Content : {message.content}')\n",
    "\n",
    "print()\n",
    "\n",
    "response = llm_chat_model.invoke(chat_message)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883e184-fab6-4201-82e5-e1f1941ca6df",
   "metadata": {},
   "source": [
    "# Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47a610c3-920d-4adb-82f2-5f1fda59349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d2f4399-be84-467f-a9d4-980b00588905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's clarify the differences between LangChain, LangSmith, and LangGraph.  They're all related to the field of large language model (LLM) application development, but they serve distinct purposes:\n",
      "\n",
      "**1. LangChain:**\n",
      "\n",
      "* **What it is:** A framework for developing applications with large language models (LLMs).  Think of it as a toolbox providing components and structures to build more complex and robust LLM-powered applications.\n",
      "* **Purpose:**  It simplifies the process of connecting LLMs to other sources of data (like databases, APIs, or your own documents) and managing the workflow of interacting with the LLM.  It handles things like prompting, memory management, chaining multiple LLMs together, and more.\n",
      "* **Key Features:**\n",
      "    * **Modular components:**  Offers pre-built modules for various tasks (e.g., prompt templates, memory mechanisms, chains for combining operations).\n",
      "    * **Integration with various LLMs:** Supports many popular LLMs (OpenAI, Hugging Face, etc.).\n",
      "    * **Data connection:**  Facilitates connecting LLMs to external data sources.\n",
      "    * **Chain management:**  Allows you to create complex workflows by chaining together different LLM calls and other operations.\n",
      "\n",
      "**2. LangSmith:**\n",
      "\n",
      "* **What it is:** A platform for debugging, testing, and evaluating LLMs and LLM applications built with frameworks like LangChain.\n",
      "* **Purpose:**  It helps developers improve the performance and reliability of their LLM applications.  Think of it as a debugging and testing environment specifically tailored for LLMs.\n",
      "* **Key Features:**\n",
      "    * **Experiment tracking:**  Logs and tracks different experiments (different prompts, LLMs, parameters, etc.).\n",
      "    * **Testing and debugging:**  Provides tools to identify and fix issues in LLM applications.\n",
      "    * **Performance analysis:** Helps analyze the performance of your LLM applications.\n",
      "    * **Integration with LangChain:**  Seamlessly integrates with LangChain, allowing you to easily track and analyze experiments built with LangChain.\n",
      "\n",
      "**3. LangGraph:**\n",
      "\n",
      "* **What it is:**  I couldn't find a widely known, established project or library called \"LangGraph\" in the context of LLM development. There might be a less popular or newly emerging project with this name, or it might be a misunderstanding or a different name altogether.  If you encountered this term in a specific context, please provide more information.\n",
      "\n",
      "\n",
      "**In short:**\n",
      "\n",
      "* **LangChain:**  Builds the LLM application.\n",
      "* **LangSmith:**  Tests, debugs, and evaluates the LLM application.\n",
      "* **LangGraph:**  (If it exists in the context you're referring to) likely involves graph-based representation or processing related to LLMs, but more information is needed to clarify its purpose.\n",
      "\n",
      "\n",
      "If you have further questions or want to explore specific aspects of LangChain or LangSmith (like code examples), let me know!\n"
     ]
    }
   ],
   "source": [
    "chain = chat_template | llm_chat_model | output_parser\n",
    "print(chain.invoke({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb58a31-ffbf-4ccd-b30c-2789b67a0b23",
   "metadata": {},
   "source": [
    "## Without using .format_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b42cd65f-8c59-46e9-82ce-238eec540fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decorators in Python are a powerful and expressive feature that allows you to modify or enhance functions and methods in a clean and readable way.  They're a form of metaprogramming, meaning they're code that manipulates other code.  Essentially, a decorator wraps another function, adding functionality before or after the wrapped function's execution without modifying its core behavior.\n",
      "\n",
      "**The Basics:**\n",
      "\n",
      "A decorator is a function that takes another function as input and returns a modified version of that function.  This modification happens without explicitly altering the original function's source code.  The syntax uses the `@` symbol placed above the function definition.\n",
      "\n",
      "```python\n",
      "def my_decorator(func):\n",
      "    def wrapper():\n",
      "        print(\"Something is happening before the function is called.\")\n",
      "        func()\n",
      "        print(\"Something is happening after the function is called.\")\n",
      "    return wrapper\n",
      "\n",
      "@my_decorator\n",
      "def say_hello():\n",
      "    print(\"Hello!\")\n",
      "\n",
      "say_hello()\n",
      "```\n",
      "\n",
      "In this example:\n",
      "\n",
      "1. `my_decorator` is the decorator function. It takes `func` (the function to be decorated) as input.\n",
      "2. `wrapper` is an inner function defined within `my_decorator`.  It executes code *before* and *after* the call to `func`.\n",
      "3. `@my_decorator` above `say_hello` is the decorator syntax.  It's equivalent to `say_hello = my_decorator(say_hello)`.  The `my_decorator` function is called with `say_hello` as its argument, and the returned `wrapper` function replaces the original `say_hello` function.\n",
      "\n",
      "The output of this code will be:\n",
      "\n",
      "```\n",
      "Something is happening before the function is called.\n",
      "Hello!\n",
      "Something is happening after the function is called.\n",
      "```\n",
      "\n",
      "**Decorators with Arguments:**\n",
      "\n",
      "Decorators can also accept arguments.  This requires an extra layer of nesting:\n",
      "\n",
      "```python\n",
      "def repeat(num_times):\n",
      "    def decorator_repeat(func):\n",
      "        def wrapper(*args, **kwargs):\n",
      "            for _ in range(num_times):\n",
      "                result = func(*args, **kwargs)\n",
      "            return result\n",
      "        return wrapper\n",
      "    return decorator_repeat\n",
      "\n",
      "@repeat(num_times=3)\n",
      "def greet(name):\n",
      "    print(f\"Hello, {name}!\")\n",
      "\n",
      "greet(\"World\")\n",
      "```\n",
      "\n",
      "Here, `repeat` is a decorator factory. It takes `num_times` as an argument and returns a decorator function (`decorator_repeat`).  This inner decorator then wraps the function (`greet`), repeating its execution.  The `*args` and `**kwargs` handle positional and keyword arguments passed to the decorated function.\n",
      "\n",
      "\n",
      "**Practical Applications:**\n",
      "\n",
      "Decorators are widely used for various purposes:\n",
      "\n",
      "* **Logging:**  Record function calls, arguments, and return values.\n",
      "* **Timing:** Measure the execution time of a function.\n",
      "* **Authentication:** Check user permissions before allowing function execution.\n",
      "* **Caching:** Store the results of expensive function calls to avoid redundant computations.\n",
      "* **Input validation:**  Ensure that the function receives valid arguments.\n",
      "* **Rate limiting:** Control the frequency of function calls.\n",
      "\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "* **Preserving Function Metadata:**  When using decorators, function metadata (like `__name__`, `__doc__`, etc.) is often lost.  This can be problematic for introspection and debugging.  The `functools.wraps` decorator helps preserve this metadata:\n",
      "\n",
      "```python\n",
      "from functools import wraps\n",
      "\n",
      "def my_decorator(func):\n",
      "    @wraps(func)\n",
      "    def wrapper():\n",
      "        # ... your code ...\n",
      "        return func()\n",
      "    return wrapper\n",
      "```\n",
      "\n",
      "* **Class Decorators:** Decorators can also be applied to classes, modifying class behavior in a similar way.\n",
      "\n",
      "* **Debugging Decorators:**  Debugging decorated functions can be slightly more challenging.  Using a debugger or adding print statements within the decorator and wrapped function can be helpful.\n",
      "\n",
      "\n",
      "In summary, decorators are a powerful tool for enhancing code readability and maintainability by separating concerns and applying common functionalities across multiple functions without modifying their core logic. They are a fundamental part of writing elegant and efficient Python code.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Directly define messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a Python programming assistant.\"),\n",
    "    HumanMessage(content=\"Explain decorators in Python in detail.\")\n",
    "]\n",
    "\n",
    "# Pass directly to the model\n",
    "response = llm_chat_model.invoke(messages)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41407cc-a705-4591-8522-e74cd6a577f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
