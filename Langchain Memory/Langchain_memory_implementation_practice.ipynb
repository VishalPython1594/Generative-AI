{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f65970-a1f5-405b-b970-43a595efc93a",
   "metadata": {},
   "source": [
    "## Steps to implement a langchain model with memory:\n",
    "\n",
    "1. Import chat model and configure the API key\n",
    "2. Create chat template\n",
    "3. Initialize the memory\n",
    "4. Create an output parser\n",
    "5. Build a chain\n",
    "6. Invoke the chain with human input and chat history\n",
    "7. Saving to memory\n",
    "8. Run step 6 and 7 in loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee466e00-5f41-4ea4-bfbb-3624238077a2",
   "metadata": {},
   "source": [
    "# Step 1: Create chat model and configure the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8ef491-ec7e-4e3d-a76a-0cea4871da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "with open(r\"C:\\Users\\DeLL\\Documents\\Data Science\\Gen AI\\Building  conversational chat bot and intro to Gemini\\Google API Key.txt\") as f:\n",
    "    key = f.read().strip()\n",
    "chat_model = ChatGoogleGenerativeAI(api_key = key, model = 'gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6069ba-1c1c-469f-83a6-b5e6347f87ab",
   "metadata": {},
   "source": [
    "# Step 2: Create chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb9d75c7-3864-470e-8659-143375ca18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18363570-e957-47bd-8d15-bde79a5fc648",
   "metadata": {},
   "source": [
    "#### Example of memory implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c762c42-dee4-4515-a787-484f7d58e76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed571f6e-187a-40c7-b5b4-025abd9dbfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b78fef8-6cf2-4e7e-bb75-9ae32cd82745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: Hi!\\nAI: Hi! what's up? How can i help you?\\nHuman: what is regression?\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key = 'chat_history')\n",
    "memory.chat_memory.add_user_message('Hi!')\n",
    "memory.chat_memory.add_ai_message(\"Hi! what's up? How can i help you?\")\n",
    "memory.chat_memory.add_user_message('what is regression?')\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd4d4e4-1db0-463f-b180-182dab729e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi!\\nAI: Hi! what's up? How can i help you?\\nHuman: what is regression?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623631b6-920a-431b-83a7-abdef6ef7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi!\n",
      "AI: Hi! what's up? How can i help you?\n",
      "Human: what is regression?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1debe43d-e1ca-41cc-9285-2b89df0910a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hi! what's up? How can i help you?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is regression?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages = True)\n",
    "memory.chat_memory.add_user_message('Hi!')\n",
    "memory.chat_memory.add_ai_message(\"Hi! what's up? How can i help you?\")\n",
    "memory.chat_memory.add_user_message('what is regression?')\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0916e656-d258-4ad3-91db-9e3a89ac9493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi! what's up? How can i help you?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is regression?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83bddc-1242-4def-bbef-f414f624a088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ab6e3-84cb-4e18-8e44-3cb5cc02cea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332d3e5-2da0-42c2-a53b-318b5eb8730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b735570-6e0d-4014-b54b-ff0ca219829a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cacccae5-ea8a-4ed8-8459-f1600c2f157d",
   "metadata": {},
   "source": [
    "*****************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960a2a3-a7f6-4b34-aad9-f8f6cf707cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19705aae-913f-4027-bc33-c8d41c23ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content = 'You are having a chatbot having a conversation with a human.'),\n",
    "    # Creating a chat history place holder\n",
    "    MessagesPlaceholder(variable_name = 'chat_history'),\n",
    "    # Human_prompt\n",
    "    HumanMessagePromptTemplate.from_template('{human_input}')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220b77d-8ab4-48d6-a632-a56594640d81",
   "metadata": {},
   "source": [
    "# Step 3: Initialize the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c008e38c-8a21-4b8d-a978-789eb4ebbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51982651-d231-4759-a480-8ce6071508cb",
   "metadata": {},
   "source": [
    "# Step 4: Create an Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8fa56e4-b7b1-4d60-ae80-6afbe706e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c9696-4bd0-4514-abfe-9ced3138f810",
   "metadata": {},
   "source": [
    "# Step 5: Build a chain (Alternative approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd50e3d-bbc7-4206-86bb-c237f9e5095a",
   "metadata": {},
   "source": [
    "### That method was called Runnable passthrough or runnable method (using '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc9c93-1985-49c9-876f-49018ca1aa9f",
   "metadata": {},
   "source": [
    "But now since we will use RAG this method (which we will see now) is more appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44d4c123-5d2c-493f-9d31-259851d3fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# define a function to load messages from memory\n",
    "def get_messages_from_memory(human_input):\n",
    "    return memory.load_memory_variables(human_input)['chat_history']\n",
    "    \n",
    "# Define a chain\n",
    "chain = RunnablePassthrough.assign(chat_history = RunnableLambda(get_messages_from_memory)) | chat_prompt_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74757a7e-5867-4784-a407-55be789d670c",
   "metadata": {},
   "source": [
    "# Step 6: Invoke the chain with human_input and chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f200de95-65c9-4d5a-9ffb-41467f7e5a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi there! I'm doing great, thanks for asking. How are you today?\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {'human_input' : 'Hi! how are you?'}\n",
    "response = chain.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5e7d7-5b18-4afc-a3db-e6f4fde34bb9",
   "metadata": {},
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "#define a function to load messages from memory\n",
    "def get_messages_from_memory(human_input):\n",
    "    return memory.buffer\n",
    "\n",
    "#Define a chain\n",
    "chain = RunnablePassthrough.assign(chat_history = RunnableLambda(get_messages_from_memory)) | chat_prompt_template | chat_model | output_parser\n",
    "\n",
    "query = {'human_input' : 'Hi! how are you?'}\n",
    "response = chain.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264ffd3-0752-4289-861b-bc99a9446553",
   "metadata": {},
   "source": [
    "# Step 7: Saving to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8f52943-1093-477c-8143-e25d0cdfef43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi! how are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi there! I'm doing great, thanks for asking. How are you today?\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context(query, {'output' : response})\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c56cb-fded-4f3b-8959-7de980e33c19",
   "metadata": {},
   "source": [
    "# Step 8: Run steps 6 and 7 in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc1e9df1-56ea-44d7-92d7-35f9c2fcc354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input : hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi\n",
      "AI : Hi!  What's up?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input : how are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: how are you?\n",
      "AI : I'm doing well, thank you for asking!  How are you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input : my name is vishal, what is your name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: my name is vishal, what is your name?\n",
      "AI : I don't have a name. I'm a large language model, an AI.  It's nice to meet you, Vishal!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input : tell me something about BERT model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: tell me something about BERT model\n",
      "AI : BERT, or Bidirectional Encoder Representations from Transformers, is a powerful natural language processing (NLP) model developed by Google.  Here's a breakdown of its key features:\n",
      "\n",
      "* **Bidirectional:** Unlike previous models that read text sequentially (left-to-right or right-to-left), BERT processes the entire sequence of words at once, considering the context from both directions. This allows it to better understand the meaning of words based on their surrounding words.\n",
      "\n",
      "* **Transformer-based:** BERT utilizes the transformer architecture, which relies on self-attention mechanisms.  This allows the model to weigh the importance of different words in a sentence when determining the meaning of a particular word.  This is a significant improvement over recurrent neural networks (RNNs) which often struggle with long sequences.\n",
      "\n",
      "* **Pre-trained:** BERT is pre-trained on a massive dataset of text and code, allowing it to learn a general understanding of language. This pre-training is crucial because it allows the model to be fine-tuned for specific NLP tasks with significantly less training data.\n",
      "\n",
      "* **Masked Language Modeling (MLM):**  During pre-training, BERT randomly masks some words in the input sequence and then tries to predict those masked words based on the context. This helps the model learn a deeper understanding of word relationships and contextual meaning.\n",
      "\n",
      "* **Next Sentence Prediction (NSP):**  Another pre-training task where BERT is trained to predict whether two given sentences follow each other in the original text. This helps the model understand sentence relationships and coherence.\n",
      "\n",
      "* **Fine-tuning:** After pre-training, BERT can be fine-tuned for various downstream NLP tasks, such as:\n",
      "    * **Question Answering:** Answering questions based on a given context.\n",
      "    * **Sentiment Analysis:** Determining the sentiment (positive, negative, neutral) expressed in a piece of text.\n",
      "    * **Text Classification:** Categorizing text into different classes.\n",
      "    * **Named Entity Recognition (NER):** Identifying and classifying named entities (people, organizations, locations, etc.) in text.\n",
      "\n",
      "\n",
      "In short, BERT's bidirectional nature, transformer architecture, and pre-training on a massive dataset make it a highly effective and versatile model for a wide range of NLP tasks.  It significantly advanced the field of NLP when it was introduced.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input : thanks, do you remember my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: thanks, do you remember my name?\n",
      "AI : Yes, you told me your name is Vishal.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input : summarize the entire conversation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: summarize the entire conversation\n",
      "AI : We started with greetings and pleasantries.  You asked about my well-being, and I reciprocated. You then introduced yourself as Vishal and asked my name (I explained I don't have one as I'm an AI).  Finally, you asked about the BERT model, and I provided a detailed explanation of its key features and applications.  We concluded with you asking if I remembered your name, which I confirmed.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input : a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: a\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = {'human_input' : input('Enter your input :' )}\n",
    "    print(f'User: {query['human_input']}')\n",
    "    if query['human_input'].lower().strip() in ['a', 'bye']:\n",
    "        break\n",
    "    response = chain.invoke(query)\n",
    "    print(f'AI : {response}')\n",
    "    memory.save_context(query, {'output' : response})\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e44ba3e-6f35-4633-94be-f9da9269b8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Hi! how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hi there! I'm doing great, thanks for asking. How are you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hi!  What's up?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='how are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking!  How are you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='my name is vishal, what is your name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't have a name. I'm a large language model, an AI.  It's nice to meet you, Vishal!\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='tell me something about BERT model', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"BERT, or Bidirectional Encoder Representations from Transformers, is a powerful natural language processing (NLP) model developed by Google.  Here's a breakdown of its key features:\\n\\n* **Bidirectional:** Unlike previous models that read text sequentially (left-to-right or right-to-left), BERT processes the entire sequence of words at once, considering the context from both directions. This allows it to better understand the meaning of words based on their surrounding words.\\n\\n* **Transformer-based:** BERT utilizes the transformer architecture, which relies on self-attention mechanisms.  This allows the model to weigh the importance of different words in a sentence when determining the meaning of a particular word.  This is a significant improvement over recurrent neural networks (RNNs) which often struggle with long sequences.\\n\\n* **Pre-trained:** BERT is pre-trained on a massive dataset of text and code, allowing it to learn a general understanding of language. This pre-training is crucial because it allows the model to be fine-tuned for specific NLP tasks with significantly less training data.\\n\\n* **Masked Language Modeling (MLM):**  During pre-training, BERT randomly masks some words in the input sequence and then tries to predict those masked words based on the context. This helps the model learn a deeper understanding of word relationships and contextual meaning.\\n\\n* **Next Sentence Prediction (NSP):**  Another pre-training task where BERT is trained to predict whether two given sentences follow each other in the original text. This helps the model understand sentence relationships and coherence.\\n\\n* **Fine-tuning:** After pre-training, BERT can be fine-tuned for various downstream NLP tasks, such as:\\n    * **Question Answering:** Answering questions based on a given context.\\n    * **Sentiment Analysis:** Determining the sentiment (positive, negative, neutral) expressed in a piece of text.\\n    * **Text Classification:** Categorizing text into different classes.\\n    * **Named Entity Recognition (NER):** Identifying and classifying named entities (people, organizations, locations, etc.) in text.\\n\\n\\nIn short, BERT's bidirectional nature, transformer architecture, and pre-training on a massive dataset make it a highly effective and versatile model for a wide range of NLP tasks.  It significantly advanced the field of NLP when it was introduced.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='thanks, do you remember my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes, you told me your name is Vishal.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='summarize the entire conversation', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"We started with greetings and pleasantries.  You asked about my well-being, and I reciprocated. You then introduced yourself as Vishal and asked my name (I explained I don't have one as I'm an AI).  Finally, you asked about the BERT model, and I provided a detailed explanation of its key features and applications.  We concluded with you asking if I remembered your name, which I confirmed.\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf9b136d-584b-4e32-aef5-802447579277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi! how are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi there! I'm doing great, thanks for asking. How are you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi!  What's up?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='how are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thank you for asking!  How are you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='my name is vishal, what is your name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I don't have a name. I'm a large language model, an AI.  It's nice to meet you, Vishal!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='tell me something about BERT model', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"BERT, or Bidirectional Encoder Representations from Transformers, is a powerful natural language processing (NLP) model developed by Google.  Here's a breakdown of its key features:\\n\\n* **Bidirectional:** Unlike previous models that read text sequentially (left-to-right or right-to-left), BERT processes the entire sequence of words at once, considering the context from both directions. This allows it to better understand the meaning of words based on their surrounding words.\\n\\n* **Transformer-based:** BERT utilizes the transformer architecture, which relies on self-attention mechanisms.  This allows the model to weigh the importance of different words in a sentence when determining the meaning of a particular word.  This is a significant improvement over recurrent neural networks (RNNs) which often struggle with long sequences.\\n\\n* **Pre-trained:** BERT is pre-trained on a massive dataset of text and code, allowing it to learn a general understanding of language. This pre-training is crucial because it allows the model to be fine-tuned for specific NLP tasks with significantly less training data.\\n\\n* **Masked Language Modeling (MLM):**  During pre-training, BERT randomly masks some words in the input sequence and then tries to predict those masked words based on the context. This helps the model learn a deeper understanding of word relationships and contextual meaning.\\n\\n* **Next Sentence Prediction (NSP):**  Another pre-training task where BERT is trained to predict whether two given sentences follow each other in the original text. This helps the model understand sentence relationships and coherence.\\n\\n* **Fine-tuning:** After pre-training, BERT can be fine-tuned for various downstream NLP tasks, such as:\\n    * **Question Answering:** Answering questions based on a given context.\\n    * **Sentiment Analysis:** Determining the sentiment (positive, negative, neutral) expressed in a piece of text.\\n    * **Text Classification:** Categorizing text into different classes.\\n    * **Named Entity Recognition (NER):** Identifying and classifying named entities (people, organizations, locations, etc.) in text.\\n\\n\\nIn short, BERT's bidirectional nature, transformer architecture, and pre-training on a massive dataset make it a highly effective and versatile model for a wide range of NLP tasks.  It significantly advanced the field of NLP when it was introduced.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks, do you remember my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes, you told me your name is Vishal.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='summarize the entire conversation', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"We started with greetings and pleasantries.  You asked about my well-being, and I reciprocated. You then introduced yourself as Vishal and asked my name (I explained I don't have one as I'm an AI).  Finally, you asked about the BERT model, and I provided a detailed explanation of its key features and applications.  We concluded with you asking if I remembered your name, which I confirmed.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2cf89-bd98-43d3-9d91-e136cac0b6c0",
   "metadata": {},
   "source": [
    "**what we have implemented so far:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d2f1bd1-68e5-401d-a190-757e3b70aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down the key differences between BERT and GPT models, focusing on how they understand and generate text.  Think of it like this: BERT is a great *understanding* machine, while GPT excels at *generation*.\n",
      "\n",
      "**BERT (Bidirectional Encoder Representations from Transformers):**\n",
      "\n",
      "* **Focus:** Understanding the context of a word by looking at the words *before and after* it simultaneously.  This is called \"bidirectional.\" Imagine you're reading a sentence – you don't just read left-to-right; you use the entire sentence's context to understand each word.  That's what BERT does.\n",
      "* **Architecture:**  It's primarily an encoder. Think of an encoder as a machine that takes input (a sentence) and transforms it into a representation that captures its meaning.  It doesn't generate new text; it analyzes existing text.\n",
      "* **Tasks:** Excellent at tasks like question answering, sentiment analysis, named entity recognition (finding names of people, places, etc.).  It understands the nuances of language exceptionally well.\n",
      "* **Example:**  Given the sentence \"The cat sat on the mat,\" BERT can understand the role of each word within the sentence's context. It can answer questions like, \"What sat on the mat?\" accurately.\n",
      "\n",
      "\n",
      "**GPT (Generative Pre-trained Transformer):**\n",
      "\n",
      "* **Focus:** Generating text.  It predicts the next word in a sequence based on the words that came before it.  This is a \"unidirectional\" approach (left-to-right). Imagine writing a story – you build it word by word, based on what you've already written.\n",
      "* **Architecture:** Primarily a decoder. A decoder takes a representation (like the meaning of a sentence) and transforms it into output (text).  It's a creative text-producing machine.\n",
      "* **Tasks:**  Excellent at text generation, translation, summarization, and creative writing.  It can produce coherent and often surprisingly human-like text.\n",
      "* **Example:** Given the prompt \"Once upon a time...\", GPT can generate a whole story based on that starting point.  It doesn't understand the story in the same way BERT does; it just predicts the next words that statistically seem to fit.\n",
      "\n",
      "\n",
      "**Here's a table summarizing the key differences:**\n",
      "\n",
      "| Feature        | BERT                               | GPT                                   |\n",
      "|----------------|------------------------------------|---------------------------------------|\n",
      "| **Directionality** | Bidirectional                       | Unidirectional (left-to-right)       |\n",
      "| **Primary Function** | Understanding text                 | Generating text                       |\n",
      "| **Architecture**   | Encoder                             | Decoder                               |\n",
      "| **Strengths**      | Contextual understanding, accuracy | Text generation, fluency               |\n",
      "| **Typical Tasks**   | Question answering, sentiment analysis | Text completion, translation, summarization |\n",
      "\n",
      "\n",
      "**In short:** BERT is a deep understanding engine, while GPT is a creative text generation engine.  They are both powerful models, but they excel at different tasks.  Many advanced models now combine aspects of both BERT and GPT architectures to leverage the strengths of each.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt here :  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : hi\n",
      "AI BOT : Hi there! How can I help you today?  Do you have any questions about data science, AI, or generative AI?  I'm ready to assist you.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt here :  what skills are required to become a gen ai engineer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : what skills are required to become a gen ai engineer\n",
      "AI BOT : Becoming a Gen AI engineer requires a blend of technical expertise, creative problem-solving, and a strong understanding of the underlying principles.  Here's a breakdown of the key skills:\n",
      "\n",
      "**Foundational Skills:**\n",
      "\n",
      "* **Strong Programming Proficiency:**  Python is the dominant language in AI, so mastery is crucial. You'll need to be comfortable with data structures, algorithms, and object-oriented programming.\n",
      "* **Mathematics and Statistics:** A solid grasp of linear algebra, calculus, probability, and statistics is fundamental.  You'll be working with large datasets and complex models, so understanding the mathematical underpinnings is essential.\n",
      "* **Machine Learning Fundamentals:**  Understanding various machine learning algorithms (supervised, unsupervised, reinforcement learning) is vital.  You should be able to choose the right algorithm for a given task and understand its limitations.\n",
      "* **Deep Learning:**  Gen AI heavily relies on deep learning techniques.  You need a deep understanding of neural networks, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers (like those used in BERT and GPT).\n",
      "\n",
      "**Specialized Gen AI Skills:**\n",
      "\n",
      "* **Natural Language Processing (NLP):** This is the core of many Gen AI applications.  You'll need to understand techniques like tokenization, word embeddings (Word2Vec, GloVe, FastText), attention mechanisms, and sequence-to-sequence models.\n",
      "* **Large Language Models (LLMs):**  Familiarity with architectures like GPT, BERT, and others is essential.  You should understand how these models are trained, fine-tuned, and deployed.\n",
      "* **Prompt Engineering:**  This is a crucial skill for getting the best results from Gen AI models.  You need to be able to craft effective prompts that guide the model to generate the desired output.\n",
      "* **Model Evaluation and Tuning:**  You need to know how to evaluate the performance of Gen AI models and fine-tune them to improve their accuracy and efficiency.  Metrics like BLEU score, ROUGE score, and perplexity are important.\n",
      "* **Deployment and Scaling:**  Understanding how to deploy Gen AI models to production environments and scale them to handle large workloads is critical. This might involve cloud computing platforms like AWS, Google Cloud, or Azure.\n",
      "\n",
      "**Soft Skills:**\n",
      "\n",
      "* **Problem-solving:**  Gen AI often involves tackling complex and ambiguous problems.  Strong problem-solving skills are essential.\n",
      "* **Creativity and Innovation:**  Gen AI is a rapidly evolving field, so creativity and the ability to think outside the box are valuable assets.\n",
      "* **Communication:**  Being able to effectively communicate technical concepts to both technical and non-technical audiences is vital.\n",
      "* **Collaboration:**  Gen AI projects often involve teamwork, so effective collaboration skills are important.\n",
      "* **Continuous Learning:** The field is constantly evolving, so a commitment to continuous learning is essential.\n",
      "\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "Start with a strong foundation in programming and mathematics. Then, focus on machine learning and deep learning courses and projects.  Explore NLP resources and work on projects involving LLMs.  Practice prompt engineering and experiment with different models.  Consider contributing to open-source projects to gain practical experience.\n",
      "\n",
      "\n",
      "This is a demanding but rewarding field.  By focusing on these skills, you'll significantly increase your chances of success as a Gen AI engineer.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt here :  my name is vishal what is your name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : my name is vishal what is your name\n",
      "AI BOT : I don't have a name. I'm a large language model, an AI.  It's nice to meet you, Vishal!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt here :  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : bye\n",
      "Bye! Have a nice day!\n"
     ]
    }
   ],
   "source": [
    "# creating model and setting api key\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "with open(r\"C:\\Users\\DeLL\\Documents\\Data Science\\Gen AI\\Building  conversational chat bot and intro to Gemini\\Google API Key.txt\") as f:\n",
    "    key = f.read().strip()\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(api_key = key, model = 'gemini-1.5-flash')\n",
    "##############\n",
    "\n",
    "# creating chat template\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content = '''Behave like a helpful AI instructor who is concerned to clear the doubts and concepts related \n",
    "    to data science, AI and Gen AI. You give the answer in a simple and intuitive manner and in such a way so that the person \n",
    "    can easily understand and appear confidently for interviews related to AI. You also give examples wherever required. Lets think step by step.'''),\n",
    "    MessagesPlaceholder(variable_name = 'chat_history'),\n",
    "    HumanMessagePromptTemplate.from_template('{human_input}')\n",
    "])\n",
    "###############\n",
    "\n",
    "#initializing memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages = True)\n",
    "##############\n",
    "\n",
    "#creating an output_parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "##############\n",
    "\n",
    "# creating chain\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "# function to load message from memory\n",
    "\n",
    "def get_message_from_memory(human_input):\n",
    "    return memory.load_memory_variables(human_input)['chat_history']\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history = RunnableLambda(get_message_from_memory)) | chat_template | chat_model | output_parser\n",
    "\n",
    "###################\n",
    "\n",
    "# Invoke the chain with human input and chat history\n",
    "query = {'human_input' : 'explain the key difference between BERT and GPT models'}\n",
    "response = chain.invoke(query)\n",
    "print(response)\n",
    "print()\n",
    "\n",
    "##################\n",
    "\n",
    "# Saving to memory\n",
    "memory.save_context(query, {'output' : response})\n",
    "\n",
    "##################\n",
    "\n",
    "# repating step 6 and 7 in loop to generate a conversational bot\n",
    "while True:\n",
    "\n",
    "    query = {'human_input' : input('Enter your prompt here : ')}\n",
    "    print(f'User : {query['human_input']}')\n",
    "    if query['human_input'] in ['bye', 'exit']:\n",
    "        print('Bye! Have a nice day!')\n",
    "        break\n",
    "    response = chain.invoke(query)\n",
    "    print(f'AI BOT : {response}')\n",
    "    print()\n",
    "    memory.save_context(query, {'output' : response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1af2cba-2f7d-4c88-a958-17e4b7e946d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='explain the key difference between BERT and GPT models', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Let\\'s break down the key differences between BERT and GPT models, focusing on how they understand and generate text.  Think of it like this: BERT is a great *understanding* machine, while GPT excels at *generation*.\\n\\n**BERT (Bidirectional Encoder Representations from Transformers):**\\n\\n* **Focus:** Understanding the context of a word by looking at the words *before and after* it simultaneously.  This is called \"bidirectional.\" Imagine you\\'re reading a sentence – you don\\'t just read left-to-right; you use the entire sentence\\'s context to understand each word.  That\\'s what BERT does.\\n* **Architecture:**  It\\'s primarily an encoder. Think of an encoder as a machine that takes input (a sentence) and transforms it into a representation that captures its meaning.  It doesn\\'t generate new text; it analyzes existing text.\\n* **Tasks:** Excellent at tasks like question answering, sentiment analysis, named entity recognition (finding names of people, places, etc.).  It understands the nuances of language exceptionally well.\\n* **Example:**  Given the sentence \"The cat sat on the mat,\" BERT can understand the role of each word within the sentence\\'s context. It can answer questions like, \"What sat on the mat?\" accurately.\\n\\n\\n**GPT (Generative Pre-trained Transformer):**\\n\\n* **Focus:** Generating text.  It predicts the next word in a sequence based on the words that came before it.  This is a \"unidirectional\" approach (left-to-right). Imagine writing a story – you build it word by word, based on what you\\'ve already written.\\n* **Architecture:** Primarily a decoder. A decoder takes a representation (like the meaning of a sentence) and transforms it into output (text).  It\\'s a creative text-producing machine.\\n* **Tasks:**  Excellent at text generation, translation, summarization, and creative writing.  It can produce coherent and often surprisingly human-like text.\\n* **Example:** Given the prompt \"Once upon a time...\", GPT can generate a whole story based on that starting point.  It doesn\\'t understand the story in the same way BERT does; it just predicts the next words that statistically seem to fit.\\n\\n\\n**Here\\'s a table summarizing the key differences:**\\n\\n| Feature        | BERT                               | GPT                                   |\\n|----------------|------------------------------------|---------------------------------------|\\n| **Directionality** | Bidirectional                       | Unidirectional (left-to-right)       |\\n| **Primary Function** | Understanding text                 | Generating text                       |\\n| **Architecture**   | Encoder                             | Decoder                               |\\n| **Strengths**      | Contextual understanding, accuracy | Text generation, fluency               |\\n| **Typical Tasks**   | Question answering, sentiment analysis | Text completion, translation, summarization |\\n\\n\\n**In short:** BERT is a deep understanding engine, while GPT is a creative text generation engine.  They are both powerful models, but they excel at different tasks.  Many advanced models now combine aspects of both BERT and GPT architectures to leverage the strengths of each.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hi there! How can I help you today?  Do you have any questions about data science, AI, or generative AI?  I'm ready to assist you.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what skills are required to become a gen ai engineer', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Becoming a Gen AI engineer requires a blend of technical expertise, creative problem-solving, and a strong understanding of the underlying principles.  Here's a breakdown of the key skills:\\n\\n**Foundational Skills:**\\n\\n* **Strong Programming Proficiency:**  Python is the dominant language in AI, so mastery is crucial. You'll need to be comfortable with data structures, algorithms, and object-oriented programming.\\n* **Mathematics and Statistics:** A solid grasp of linear algebra, calculus, probability, and statistics is fundamental.  You'll be working with large datasets and complex models, so understanding the mathematical underpinnings is essential.\\n* **Machine Learning Fundamentals:**  Understanding various machine learning algorithms (supervised, unsupervised, reinforcement learning) is vital.  You should be able to choose the right algorithm for a given task and understand its limitations.\\n* **Deep Learning:**  Gen AI heavily relies on deep learning techniques.  You need a deep understanding of neural networks, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers (like those used in BERT and GPT).\\n\\n**Specialized Gen AI Skills:**\\n\\n* **Natural Language Processing (NLP):** This is the core of many Gen AI applications.  You'll need to understand techniques like tokenization, word embeddings (Word2Vec, GloVe, FastText), attention mechanisms, and sequence-to-sequence models.\\n* **Large Language Models (LLMs):**  Familiarity with architectures like GPT, BERT, and others is essential.  You should understand how these models are trained, fine-tuned, and deployed.\\n* **Prompt Engineering:**  This is a crucial skill for getting the best results from Gen AI models.  You need to be able to craft effective prompts that guide the model to generate the desired output.\\n* **Model Evaluation and Tuning:**  You need to know how to evaluate the performance of Gen AI models and fine-tune them to improve their accuracy and efficiency.  Metrics like BLEU score, ROUGE score, and perplexity are important.\\n* **Deployment and Scaling:**  Understanding how to deploy Gen AI models to production environments and scale them to handle large workloads is critical. This might involve cloud computing platforms like AWS, Google Cloud, or Azure.\\n\\n**Soft Skills:**\\n\\n* **Problem-solving:**  Gen AI often involves tackling complex and ambiguous problems.  Strong problem-solving skills are essential.\\n* **Creativity and Innovation:**  Gen AI is a rapidly evolving field, so creativity and the ability to think outside the box are valuable assets.\\n* **Communication:**  Being able to effectively communicate technical concepts to both technical and non-technical audiences is vital.\\n* **Collaboration:**  Gen AI projects often involve teamwork, so effective collaboration skills are important.\\n* **Continuous Learning:** The field is constantly evolving, so a commitment to continuous learning is essential.\\n\\n\\n**Getting Started:**\\n\\nStart with a strong foundation in programming and mathematics. Then, focus on machine learning and deep learning courses and projects.  Explore NLP resources and work on projects involving LLMs.  Practice prompt engineering and experiment with different models.  Consider contributing to open-source projects to gain practical experience.\\n\\n\\nThis is a demanding but rewarding field.  By focusing on these skills, you'll significantly increase your chances of success as a Gen AI engineer.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='my name is vishal what is your name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't have a name. I'm a large language model, an AI.  It's nice to meet you, Vishal!\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f991a1-3564-46b3-bd00-a3aafe57959d",
   "metadata": {},
   "source": [
    "### Now whenever we turn off the system or disconnect from internet all this history gets deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6a850-7c14-4035-8328-3d972b4f8023",
   "metadata": {},
   "source": [
    "# Saving chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b863e89e-a1c3-47b8-b1c8-f4e64145374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de8064fe-fbfc-4026-a3af-812355e7e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = pickle.dumps(memory)\n",
    "with open('conversation_memory.pkl','wb') as f:\n",
    "    f.write(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665f50b-cb85-435d-bd8a-4c9a8ced7229",
   "metadata": {},
   "source": [
    "# Loading chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9cd3f1d-8329-45e3-b9cf-45986878299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='explain the key difference between BERT and GPT models', additional_kwargs={}, response_metadata={}), AIMessage(content='Let\\'s break down the key differences between BERT and GPT models, focusing on how they understand and generate text.  Think of it like this: BERT is a great *understanding* machine, while GPT excels at *generation*.\\n\\n**BERT (Bidirectional Encoder Representations from Transformers):**\\n\\n* **Focus:** Understanding the context of a word by looking at the words *before and after* it simultaneously.  This is called \"bidirectional.\" Imagine you\\'re reading a sentence – you don\\'t just read left-to-right; you use the entire sentence\\'s context to understand each word.  That\\'s what BERT does.\\n* **Architecture:**  It\\'s primarily an encoder. Think of an encoder as a machine that takes input (a sentence) and transforms it into a representation that captures its meaning.  It doesn\\'t generate new text; it analyzes existing text.\\n* **Tasks:** Excellent at tasks like question answering, sentiment analysis, named entity recognition (finding names of people, places, etc.).  It understands the nuances of language exceptionally well.\\n* **Example:**  Given the sentence \"The cat sat on the mat,\" BERT can understand the role of each word within the sentence\\'s context. It can answer questions like, \"What sat on the mat?\" accurately.\\n\\n\\n**GPT (Generative Pre-trained Transformer):**\\n\\n* **Focus:** Generating text.  It predicts the next word in a sequence based on the words that came before it.  This is a \"unidirectional\" approach (left-to-right). Imagine writing a story – you build it word by word, based on what you\\'ve already written.\\n* **Architecture:** Primarily a decoder. A decoder takes a representation (like the meaning of a sentence) and transforms it into output (text).  It\\'s a creative text-producing machine.\\n* **Tasks:**  Excellent at text generation, translation, summarization, and creative writing.  It can produce coherent and often surprisingly human-like text.\\n* **Example:** Given the prompt \"Once upon a time...\", GPT can generate a whole story based on that starting point.  It doesn\\'t understand the story in the same way BERT does; it just predicts the next words that statistically seem to fit.\\n\\n\\n**Here\\'s a table summarizing the key differences:**\\n\\n| Feature        | BERT                               | GPT                                   |\\n|----------------|------------------------------------|---------------------------------------|\\n| **Directionality** | Bidirectional                       | Unidirectional (left-to-right)       |\\n| **Primary Function** | Understanding text                 | Generating text                       |\\n| **Architecture**   | Encoder                             | Decoder                               |\\n| **Strengths**      | Contextual understanding, accuracy | Text generation, fluency               |\\n| **Typical Tasks**   | Question answering, sentiment analysis | Text completion, translation, summarization |\\n\\n\\n**In short:** BERT is a deep understanding engine, while GPT is a creative text generation engine.  They are both powerful models, but they excel at different tasks.  Many advanced models now combine aspects of both BERT and GPT architectures to leverage the strengths of each.', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi there! How can I help you today?  Do you have any questions about data science, AI, or generative AI?  I'm ready to assist you.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='what skills are required to become a gen ai engineer', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Becoming a Gen AI engineer requires a blend of technical expertise, creative problem-solving, and a strong understanding of the underlying principles.  Here's a breakdown of the key skills:\\n\\n**Foundational Skills:**\\n\\n* **Strong Programming Proficiency:**  Python is the dominant language in AI, so mastery is crucial. You'll need to be comfortable with data structures, algorithms, and object-oriented programming.\\n* **Mathematics and Statistics:** A solid grasp of linear algebra, calculus, probability, and statistics is fundamental.  You'll be working with large datasets and complex models, so understanding the mathematical underpinnings is essential.\\n* **Machine Learning Fundamentals:**  Understanding various machine learning algorithms (supervised, unsupervised, reinforcement learning) is vital.  You should be able to choose the right algorithm for a given task and understand its limitations.\\n* **Deep Learning:**  Gen AI heavily relies on deep learning techniques.  You need a deep understanding of neural networks, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers (like those used in BERT and GPT).\\n\\n**Specialized Gen AI Skills:**\\n\\n* **Natural Language Processing (NLP):** This is the core of many Gen AI applications.  You'll need to understand techniques like tokenization, word embeddings (Word2Vec, GloVe, FastText), attention mechanisms, and sequence-to-sequence models.\\n* **Large Language Models (LLMs):**  Familiarity with architectures like GPT, BERT, and others is essential.  You should understand how these models are trained, fine-tuned, and deployed.\\n* **Prompt Engineering:**  This is a crucial skill for getting the best results from Gen AI models.  You need to be able to craft effective prompts that guide the model to generate the desired output.\\n* **Model Evaluation and Tuning:**  You need to know how to evaluate the performance of Gen AI models and fine-tune them to improve their accuracy and efficiency.  Metrics like BLEU score, ROUGE score, and perplexity are important.\\n* **Deployment and Scaling:**  Understanding how to deploy Gen AI models to production environments and scale them to handle large workloads is critical. This might involve cloud computing platforms like AWS, Google Cloud, or Azure.\\n\\n**Soft Skills:**\\n\\n* **Problem-solving:**  Gen AI often involves tackling complex and ambiguous problems.  Strong problem-solving skills are essential.\\n* **Creativity and Innovation:**  Gen AI is a rapidly evolving field, so creativity and the ability to think outside the box are valuable assets.\\n* **Communication:**  Being able to effectively communicate technical concepts to both technical and non-technical audiences is vital.\\n* **Collaboration:**  Gen AI projects often involve teamwork, so effective collaboration skills are important.\\n* **Continuous Learning:** The field is constantly evolving, so a commitment to continuous learning is essential.\\n\\n\\n**Getting Started:**\\n\\nStart with a strong foundation in programming and mathematics. Then, focus on machine learning and deep learning courses and projects.  Explore NLP resources and work on projects involving LLMs.  Practice prompt engineering and experiment with different models.  Consider contributing to open-source projects to gain practical experience.\\n\\n\\nThis is a demanding but rewarding field.  By focusing on these skills, you'll significantly increase your chances of success as a Gen AI engineer.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='my name is vishal what is your name', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't have a name. I'm a large language model, an AI.  It's nice to meet you, Vishal!\", additional_kwargs={}, response_metadata={})]) return_messages=True memory_key='chat_history'\n"
     ]
    }
   ],
   "source": [
    "chat_history_loaded = pickle.load(open('conversation_memory.pkl','rb'))\n",
    "print(chat_history_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62612bc3-35ba-4edc-9ff3-b3f0dcd8f45f",
   "metadata": {},
   "source": [
    "# SQL chat message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f1548ce-6f9e-4848-a3fd-c8030d8df7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain_community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "322c10e6-0a3e-4f0c-87c1-0fc973d747d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "def get_session_message_history_from_db(session_id):\n",
    "    chat_message_history = SQLChatMessageHistory(session_id = session_id, connection = 'sqlite:///chats_data/sqlite.db')\n",
    "    return chat_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15515fcc-e5ce-4a4f-8065-e3488e72c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([('system', 'You are a helpful AI assistant'),\n",
    "                                                  MessagesPlaceholder(variable_name = 'history'),\n",
    "                                                  ('user', '{user_input}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4648f502-d4a7-40ba-8e93-00d5b9a08123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain\n",
    "\n",
    "chain = chat_template | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d7566fb-e1b7-4ebe-97d4-1b2cc8eaff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#define a chain\n",
    "\n",
    "conversation_chain = RunnableWithMessageHistory(chain,\n",
    "                                                get_session_message_history_from_db,\n",
    "                                                input_messages_key = 'user_input',\n",
    "                                                history_messages_key = 'history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f330a0b1-fa67-4b44-a437-4281172fdf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George Washington'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is where we configure the session id\n",
    "user_id = 'vishal'\n",
    "config = {'configurable' : {'session_id' : user_id}}\n",
    "input_prompt = {'user_input' : 'what is the name of the first US president?'}\n",
    "response = conversation_chain.invoke(input = input_prompt, config = config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "220ebf0a-982e-4d92-bd38-30d99e73355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using database at C:\\Users\\DeLL\\Documents\\Data Science\\Gen AI\\17th Oct 2024\\data\\sqlite.db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'George Washington'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "def get_session_message_history_from_db(session_id):\n",
    "    import os\n",
    "    os.makedirs('data', exist_ok=True)  # Ensure directory exists\n",
    "    db_path = os.path.abspath('data/sqlite.db')\n",
    "    print(f\"Using database at {db_path}\")\n",
    "    chat_message_history = SQLChatMessageHistory(session_id = session_id, connection = f'sqlite:///{db_path}')\n",
    "    return chat_message_history\n",
    "\n",
    "# Create a chat template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([('system', 'You are a helpful AI assistant'),\n",
    "                                                  MessagesPlaceholder(variable_name = 'history'),\n",
    "                                                  ('user', '{user_input}')])\n",
    "\n",
    "#chain\n",
    "\n",
    "chain = chat_template | chat_model\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#define a chain\n",
    "\n",
    "conversation_chain = RunnableWithMessageHistory(chain,\n",
    "                                                get_session_message_history_from_db,\n",
    "                                                input_messages_key = 'user_input',\n",
    "                                                history_messages_key = 'history')\n",
    "\n",
    "# This is where we configure the session id\n",
    "user_id = 'vishal'\n",
    "config = {'configurable' : {'session_id' : user_id}}\n",
    "input_prompt = {'user_input' : 'what is the name of the first US president?'}\n",
    "response = conversation_chain.invoke(input = input_prompt, config = config)\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dbde4a53-6c4a-494a-83b6-0166c39d833b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George Washington'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "def get_session_message_history_from_db(session_id):\n",
    "    chat_message_history = SQLChatMessageHistory(session_id = session_id, connection = 'sqlite:///sqlite_test/sqlite.db')\n",
    "    return chat_message_history\n",
    "\n",
    "# Create a chat template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([('system', 'You are a helpful AI assistant'),\n",
    "                                                  MessagesPlaceholder(variable_name = 'history'),\n",
    "                                                  ('user', '{user_input}')])\n",
    "\n",
    "#chain\n",
    "\n",
    "chain = chat_template | chat_model\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#define a chain\n",
    "\n",
    "conversation_chain = RunnableWithMessageHistory(chain,\n",
    "                                                get_session_message_history_from_db,\n",
    "                                                input_messages_key = 'user_input',\n",
    "                                                history_messages_key = 'history')\n",
    "\n",
    "# This is where we configure the session id\n",
    "user_id = 'vishal'\n",
    "config = {'configurable' : {'session_id' : user_id}}\n",
    "input_prompt = {'user_input' : 'what is the name of the first US president?'}\n",
    "response = conversation_chain.invoke(input = input_prompt, config = config)\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4a4436cc-1808-4366-93f2-66ec42a1baa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George Washington'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 'nikita'\n",
    "config = {'configurable' : {'session_id' : user_id}}\n",
    "input_prompt = {'user_input' : 'what is the name of the first US president?'}\n",
    "response = conversation_chain.invoke(input = input_prompt, config = config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b33a443c-03be-46fd-acf4-c8ac964ae3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_bot(session_id, prompt):\n",
    "\n",
    "    config = {'configurable' : {'session_id' : user_id}}\n",
    "    input_prompt = {'user_input' : prompt}\n",
    "    response = conversation_chain.invoke(input = input_prompt, config = config)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eaef4032-04e9-483e-86c4-0893db456cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Alaska is Juneau.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_bot('vishal', 'my name is vishal. what is the capital of alaska?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf740508-07c4-4b8d-8d2b-89c6d2ee80ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your name is Vishal.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_bot('vishal', 'do you remember my name?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0165f7ce-a411-49fb-a479-18841c024498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Vikas,\n",
      "\n",
      "The Union Territories of India are:\n",
      "\n",
      "* Jammu and Kashmir\n",
      "* Ladakh\n",
      "* Delhi\n",
      "* Puducherry\n",
      "* Chandigarh\n",
      "* Dadra and Nagar Haveli and Daman and Diu\n",
      "* Lakshadweep\n",
      "* Andaman and Nicobar Islands\n",
      "\n",
      "Please note that the status and names of these territories can sometimes be subject to change due to political developments.  This list is accurate as of October 26, 2023.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "# creating function to get message history\n",
    "def get_message_history_from_db(session_id):\n",
    "    chat_history = SQLChatMessageHistory(session_id = session_id, connection = 'sqlite:///sqlite_test/sqlite.db')\n",
    "    return chat_history\n",
    "\n",
    "#creating template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "chat_template = ChatPromptTemplate.from_messages([('system', 'you are a helpful AI assistant'),\n",
    "                                                  MessagesPlaceholder(variable_name = 'history'),\n",
    "                                                  ('human', '{human_input}')])\n",
    "chain = chat_template | chat_model\n",
    "\n",
    "#bringing the memory and the chain (pipeline) together\n",
    "\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "conversational_chain = RunnableWithMessageHistory(chain,\n",
    "                                                  get_message_history_from_db,\n",
    "                                                  input_messages_key = 'human_input',\n",
    "                                                  history_messages_key = 'history')\n",
    "#configuring the session id\n",
    "\n",
    "user_id = 'vikas'\n",
    "config = {'configurable' : {'session_id' : user_id}}\n",
    "input_prompt = {'human_input' : 'my name is vikas. what are the names of union territories of india?'}\n",
    "response = conversational_chain.invoke(input_prompt, config = config)\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe05be-1c30-4149-a3c0-ae1dd1c922c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986bb0f9-75e7-493b-8b39-f176157c91ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt here :  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot : Hi Vishal! How are you doing today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt here :  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye! Have a good day!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "# Retrieves the history for a particular session using session id\n",
    "def get_messages_history_from_db(session_id):\n",
    "    chat_history = SQLChatMessageHistory(session_id = session_id, connection = 'sqlite:///sqlite_test/sqlite.db')\n",
    "    return chat_history\n",
    "\n",
    "# Creating a chat template\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "chat_template = ChatPromptTemplate.from_messages([('system', 'You are a helpful AI assistant and your name is Freddy'),\n",
    "                                                  MessagesPlaceholder(variable_name = 'history'),\n",
    "                                                  ('human', '{human_input}')])\n",
    "\n",
    "# creating a chain\n",
    "chain = chat_template | chat_model\n",
    "\n",
    "#bringing the memory and the chain (pipeline) together\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "conversational_chain = RunnableWithMessageHistory(chain,\n",
    "                                                  get_messages_history_from_db,\n",
    "                                                  history_messages_key = 'history',\n",
    "                                                  input_messages_key = 'human_input')\n",
    "\n",
    "#configuring the session id and invoking the chain\n",
    "\n",
    "user_id = 'vishal'\n",
    "config = {'configurable' : {'session_id' : user_id}}\n",
    "\n",
    "# implementing a conversational bot\n",
    "\n",
    "while True:\n",
    "    user_prompt = input('Enter your prompt here : ')\n",
    "    if user_prompt.lower().strip() in ['bye', 'exit']:\n",
    "        print('Bye! Have a good day!')\n",
    "        break\n",
    "    input_prompt = {'human_input' : user_prompt}\n",
    "    response = conversational_chain.invoke(input_prompt, config = config)\n",
    "    print(f'Bot : {response.content}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df4289c-47e2-4c63-b27b-42988c8c9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db23e03d-a5f5-4619-8b25-c03ee271d9a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m connection \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeLL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData Science\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGen AI\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m17th Oct 2024\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msqlite_test\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msqlite.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "connection = sqlite3.connect(r\"C:\\Users\\DeLL\\Documents\\Data Science\\Gen AI\\17th Oct 2024\\sqlite_test\\sqlite.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7762329c-2c95-4e2b-9881-55046e6755b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: [('message_store',)]\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = connection.execute(query).fetchall()\n",
    "\n",
    "# Print all table names\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b90e913-8dc9-4c9b-903b-d408f65f4e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"what is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"George Was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"what is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"George Was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"what is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"Juneau\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"do you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"No, I don'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"what is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"Juneau\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"do you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"No, I don'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"my name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"The capita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"do you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>nikita</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"You told m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>vikas</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"my name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>vikas</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"Hi Vikas,\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"hi\", \"a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"Hi there! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"what is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"My name is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"do you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"No, I don'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"what is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"48379\", \"a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"my name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"It's nice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"what is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"RAG stands...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"do you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"Yes, you t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"can you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"Our conver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"human\", \"data\": {\"content\": \"hi\", \"a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>vishal</td>\n",
       "      <td>{\"type\": \"ai\", \"data\": {\"content\": \"Hi Vishal!...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id session_id                                            message\n",
       "0    1     vishal  {\"type\": \"human\", \"data\": {\"content\": \"what is...\n",
       "1    2     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"George Was...\n",
       "2    3     nikita  {\"type\": \"human\", \"data\": {\"content\": \"what is...\n",
       "3    4     nikita  {\"type\": \"ai\", \"data\": {\"content\": \"George Was...\n",
       "4    5     vishal  {\"type\": \"human\", \"data\": {\"content\": \"what is...\n",
       "5    6     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"Juneau\", \"...\n",
       "6    7     vishal  {\"type\": \"human\", \"data\": {\"content\": \"do you ...\n",
       "7    8     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"No, I don'...\n",
       "8    9     nikita  {\"type\": \"human\", \"data\": {\"content\": \"what is...\n",
       "9   10     nikita  {\"type\": \"ai\", \"data\": {\"content\": \"Juneau\", \"...\n",
       "10  11     nikita  {\"type\": \"human\", \"data\": {\"content\": \"do you ...\n",
       "11  12     nikita  {\"type\": \"ai\", \"data\": {\"content\": \"No, I don'...\n",
       "12  13     nikita  {\"type\": \"human\", \"data\": {\"content\": \"my name...\n",
       "13  14     nikita  {\"type\": \"ai\", \"data\": {\"content\": \"The capita...\n",
       "14  15     nikita  {\"type\": \"human\", \"data\": {\"content\": \"do you ...\n",
       "15  16     nikita  {\"type\": \"ai\", \"data\": {\"content\": \"You told m...\n",
       "16  17      vikas  {\"type\": \"human\", \"data\": {\"content\": \"my name...\n",
       "17  18      vikas  {\"type\": \"ai\", \"data\": {\"content\": \"Hi Vikas,\\...\n",
       "18  19     vishal  {\"type\": \"human\", \"data\": {\"content\": \"hi\", \"a...\n",
       "19  20     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"Hi there! ...\n",
       "20  21     vishal  {\"type\": \"human\", \"data\": {\"content\": \"what is...\n",
       "21  22     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"My name is...\n",
       "22  23     vishal  {\"type\": \"human\", \"data\": {\"content\": \"do you ...\n",
       "23  24     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"No, I don'...\n",
       "24  25     vishal  {\"type\": \"human\", \"data\": {\"content\": \"what is...\n",
       "25  26     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"48379\", \"a...\n",
       "26  27     vishal  {\"type\": \"human\", \"data\": {\"content\": \"my name...\n",
       "27  28     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"It's nice ...\n",
       "28  29     vishal  {\"type\": \"human\", \"data\": {\"content\": \"what is...\n",
       "29  30     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"RAG stands...\n",
       "30  31     vishal  {\"type\": \"human\", \"data\": {\"content\": \"do you ...\n",
       "31  32     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"Yes, you t...\n",
       "32  33     vishal  {\"type\": \"human\", \"data\": {\"content\": \"can you...\n",
       "33  34     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"Our conver...\n",
       "34  35     vishal  {\"type\": \"human\", \"data\": {\"content\": \"hi\", \"a...\n",
       "35  36     vishal  {\"type\": \"ai\", \"data\": {\"content\": \"Hi Vishal!..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql_query('SELECT * FROM message_store', connection)\n",
    "connection.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190abec5-82a1-4ca9-b3b7-62746cd95a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['session_id'] == 'vishal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dde7b71-1475-4557-9b79-346dc16c8b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['{\"type\": \"human\", \"data\": {\"content\": \"what is the name of the first US president?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"George Washington\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-50e75841-41e2-4680-b6d4-6f8ab9068105-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 17, \"output_tokens\": 3, \"total_tokens\": 20, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"what is the capital of alaska?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"Juneau\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-978b3d49-44cf-4b2c-9229-36af1d4a81de-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 28, \"output_tokens\": 3, \"total_tokens\": 31, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"do you remember my name?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"No, I don\\'t remember your name.  As a large language model, I have no memory of past conversations.  Each interaction with me starts fresh.\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-fea0bdc7-dfdf-446a-bea2-629541180455-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 38, \"output_tokens\": 34, \"total_tokens\": 72, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"hi\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"Hi there! How can I help you today?\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-f7a4a42c-bf8e-47cd-b014-2263a33bd56c-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 79, \"output_tokens\": 11, \"total_tokens\": 90, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"what is your name?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"My name is Freddy.\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-d628ebc6-79c9-4ca2-a84f-06975904572f-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 96, \"output_tokens\": 6, \"total_tokens\": 102, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"do you remember my name?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"No, I don\\'t remember your name.  I have no memory from previous conversations.\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-2ea2a651-1b25-47fa-b049-fbaa84768c80-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 109, \"output_tokens\": 20, \"total_tokens\": 129, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"what is 59*821\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"48379\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-efa04792-5696-4fab-81d4-5344d801c005-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 139, \"output_tokens\": 6, \"total_tokens\": 145, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"my name is vishal\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"It\\'s nice to meet you, Vishal!  How can I help you?\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-b497bc6d-1f6a-4ac4-86b3-01e26208d111-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 152, \"output_tokens\": 19, \"total_tokens\": 171, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"what is RAG?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"RAG stands for **Retrieval Augmented Generation**.  It\\'s a technique used in large language models (like me!) to improve the accuracy and relevance of their responses.\\\\n\\\\nInstead of relying solely on the knowledge stored within the model\\'s parameters, RAG systems incorporate an external knowledge base.  When a question is asked, the model first retrieves relevant information from this knowledge base. This retrieved information is then used to augment the model\\'s generation process, leading to answers that are:\\\\n\\\\n* **More accurate:** Because they\\'re grounded in external facts.\\\\n* **More up-to-date:**  The knowledge base can be easily updated, unlike the model\\'s internal parameters which require extensive retraining.\\\\n* **More factual:** Less prone to hallucinations (making things up).\\\\n\\\\nThink of it like this:  without RAG, I\\'m answering based on what I\\'ve learned during my training. With RAG, I can consult a specific document or database before answering, ensuring my response is based on the most current and relevant information.\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-9fe062e5-14ad-4126-ac1f-407f5c8f4adf-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 176, \"output_tokens\": 211, \"total_tokens\": 387, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"do you remember my name?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"Yes, you told me your name is Vishal.\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-1ae695ab-e667-450d-aca4-8ed0f298348a-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 394, \"output_tokens\": 12, \"total_tokens\": 406, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"can you summarize our conversation?\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"Our conversation started with general questions: the first US president, the capital of Alaska.  We then established that I, Freddy, have no memory between sessions. You introduced yourself as Vishal. We discussed what RAG (Retrieval Augmented Generation) is, and finally, you asked me to summarize our conversation, which I am now doing.\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-767dc8c1-15e9-439f-9b4b-b4b9456edccb-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 413, \"output_tokens\": 68, \"total_tokens\": 481, \"input_token_details\": {\"cache_read\": 0}}}}',\n",
       "       '{\"type\": \"human\", \"data\": {\"content\": \"hi\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}',\n",
       "       '{\"type\": \"ai\", \"data\": {\"content\": \"Hi Vishal! How are you doing today?\", \"additional_kwargs\": {}, \"response_metadata\": {\"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []}, \"finish_reason\": \"STOP\", \"safety_ratings\": []}, \"type\": \"ai\", \"name\": null, \"id\": \"run-e3d60eef-891d-4e19-b621-82be00636c3d-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 483, \"output_tokens\": 11, \"total_tokens\": 494, \"input_token_details\": {\"cache_read\": 0}}}}'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c88562f0-2afb-4d88-96ba-179fc6c2b0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the name of the first US president?\n",
      "George Washington\n",
      "what is the capital of alaska?\n",
      "Juneau\n",
      "do you remember my name?\n",
      "No, I don't remember your name.  As a large language model, I have no memory of past conversations.  Each interaction with me starts fresh.\n",
      "hi\n",
      "Hi there! How can I help you today?\n",
      "what is your name?\n",
      "My name is Freddy.\n",
      "do you remember my name?\n",
      "No, I don't remember your name.  I have no memory from previous conversations.\n",
      "what is 59*821\n",
      "48379\n",
      "my name is vishal\n",
      "It's nice to meet you, Vishal!  How can I help you?\n",
      "what is RAG?\n",
      "RAG stands for **Retrieval Augmented Generation**.  It's a technique used in large language models (like me!) to improve the accuracy and relevance of their responses.\n",
      "\n",
      "Instead of relying solely on the knowledge stored within the model's parameters, RAG systems incorporate an external knowledge base.  When a question is asked, the model first retrieves relevant information from this knowledge base. This retrieved information is then used to augment the model's generation process, leading to answers that are:\n",
      "\n",
      "* **More accurate:** Because they're grounded in external facts.\n",
      "* **More up-to-date:**  The knowledge base can be easily updated, unlike the model's internal parameters which require extensive retraining.\n",
      "* **More factual:** Less prone to hallucinations (making things up).\n",
      "\n",
      "Think of it like this:  without RAG, I'm answering based on what I've learned during my training. With RAG, I can consult a specific document or database before answering, ensuring my response is based on the most current and relevant information.\n",
      "do you remember my name?\n",
      "Yes, you told me your name is Vishal.\n",
      "can you summarize our conversation?\n",
      "Our conversation started with general questions: the first US president, the capital of Alaska.  We then established that I, Freddy, have no memory between sessions. You introduced yourself as Vishal. We discussed what RAG (Retrieval Augmented Generation) is, and finally, you asked me to summarize our conversation, which I am now doing.\n",
      "hi\n",
      "Hi Vishal! How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Example array\n",
    "data = df['message'].values\n",
    "\n",
    "# Extract the content part\n",
    "contents = [json.loads(entry)['data']['content'] for entry in data]\n",
    "\n",
    "# Display the content\n",
    "for content in contents:\n",
    "    print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d521f-1663-4554-b1e4-046ca7924ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
